# 🤖 Agent Experiments 2025
**Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.**

A hands-on exploration of AI Agent frameworks, documenting practical implementations and insights.


## 🎯 Overview

Testing and comparing mainstream agent frameworks:
- AutoGen (Microsoft)
- PhiData
- Crew AI
- Dify

Each framework is explored through both code implementation and UI interfaces.

## 🚀 What's Inside

Currently exploring:
- AutoGen - Microsoft's flexible framework for automated task completion
- PhiData - Python-first agent development platform
- Crew AI - Multi-agent orchestration framework
- Dify - No-code AI agent development platform

Each framework is tested through both:
- 💻 Code Implementation
- 🖥️ UI Interface Exploration

Each framework section includes:
- Code examples in real-world use cases
- Performance analysis
- Personal insights and recommendations
- Common pitfalls and solutions

## 🔄 Current Status

Active project with regular updates. Focus on providing practical insights for beginners in AI Agent engineering.

## ⭐ Support

If you find this helpful, give it a star! Let's build a better understanding of AI agents together.

