# ğŸ¤– Agent Experiments 2025
**Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.**

A hands-on exploration of AI Agent frameworks, documenting practical implementations and insights.


## ğŸ¯ Overview

Testing and comparing mainstream agent frameworks:
- AutoGen (Microsoft)
- PhiData
- Crew AI
- Dify

Each framework is explored through both code implementation and UI interfaces.

## ğŸš€ What's Inside

Currently exploring:
- AutoGen - Microsoft's flexible framework for automated task completion
- PhiData - Python-first agent development platform
- Crew AI - Multi-agent orchestration framework
- Dify - No-code AI agent development platform

Each framework is tested through both:
- ğŸ’» Code Implementation
- ğŸ–¥ï¸ UI Interface Exploration

Each framework section includes:
- Code examples in real-world use cases
- Performance analysis
- Personal insights and recommendations
- Common pitfalls and solutions

## ğŸ”„ Current Status

Active project with regular updates. Focus on providing practical insights for beginners in AI Agent engineering.

## â­ Support

If you find this helpful, give it a star! Let's build a better understanding of AI agents together.

